[["index.html", "Modelación de nichos ecológicos y aŕeas de distribución con MPPs 1 Preámbulo", " Modelación de nichos ecológicos y aŕeas de distribución con MPPs Gerardo Martín 2022-08-30 1 Preámbulo En este curso aprenderemos a utilizar algunas herramientas para el análisis de datos de ocurrencia geográfica de especies y desarrollar lo que en ecología se llama Modelos de nicho ecológico y de Distribución geográfica de especies. "],["encuadre-de-la-materia.html", " 2 Encuadre de la materia 2.1 Criterios de evaluación 2.2 ¿Cómo se darán las clases? 2.3 Contacto", " 2 Encuadre de la materia 2.1 Criterios de evaluación Las constribuciones a cada calificación parcial serán: Asistencia (25%) Trabajos de clase cumplidos (50%) Examen (25%) Participación (2 puntos extra máximo) Cabe señalar, que la asistencia no corresponderá con su presencia en las sesiones sincrónicas, sino con el cumplimiento de los trabajos de clase. La participación se medirá tanto por participación directa en las sesiones sincrónicas como por el seguimiento que uds den a la clase por correo electrónico. 2.2 ¿Cómo se darán las clases? Todos los contenidos del curso, lecturas y presentaciones, se irán añadiendo a este sitio web conforme avanza el semestre. En el Google Classroom de la materia se irán anunciando las diferentes actividades y sesiones sincrónicas con anticipación suficiente. Igualmente, los examenes y resultados serán publicados a través de esta plataforma. A petición de uds, también se publicarán aquí los videos de las sesiones sincrónicas que tengamos, en especial para aquellos temas que sean mayor interés/dificultad/importancia. Los trabajos de práctica también se publicarán en Classroom. 2.3 Contacto Para reportar fallos, resolver dudas y peticiones especiales grupales o individuales por favor enviar correo electrónico a gerardo.mmc@enesmerida.unam.mx. "],["unidad-i-introducción-al-modelado-de-nichos-ecológicos.html", " 3 Unidad I: Introducción al modelado de nichos ecológicos 3.1 Introducción al modelado de nichos ecológicos 3.2 Insumos para el modelado correlativo de nichos ecológicos 3.3 El proceso de calibración 3.4 Uso de R como Sistema de Información Geográfica", " 3 Unidad I: Introducción al modelado de nichos ecológicos 3.1 Introducción al modelado de nichos ecológicos La modelación correlativa de nichos ecológicos y áreas de distribución consiste del análisis de las condiciones ambientales de las regiones donde ocurre un organismo. Frecuentemente el análisis de dichas condiciones se realiza con el fin de representarlas en un producto cartográfico. La representación cartográfica de la relación entre un organismo y el medio ambiente es posible gracias a que todas las condiciones ambientales están expresadas en la geografía. Por ejemplo, la temperatura media anual presenta patrones geográficos evidentes a lo largo y ancho de la geografía mexicana. Figura 3.1: Temperatura anual promedio en una capa ráster de México. Los modelos de nicho ecológico tienen fuertes bases biológicas, sentadas principalmente en el fenómeno de tolerancia fisiológica, la cual se refiere a que todos los organismos tienen la capacidad de soportar ciertas condiciones climatológicas. La tolerancia térmica, por ejemplo se refiere a los límites de temperatura a los cuales un organismo moriría de choque de calor o hipotermia. Por otro lado, los requerimientos hídricos podrían estar relacionados con la cantidad de agua que el organismo en cuestión necesita ingerir, y la tasa de evapotranspiración se refiere a la rapidez con que pierde agua a una temperatura determinada. Como es evidente, todas las dimensiones de la tolerancia fisiológica pueden estar interconectadas y son sumamente complejas. En realidad, los organismos son afectados por muchas fuerzas que se expresan en la geografía, desde el clima, la topografía, química del suelo, el hábitat primario (agua o tierra) y otros organismos. Las diferentes clases de influencias del medio ambiente sobre la distribución espacial de un organismo se clasifican como (Soberón and Peterson 2005): Bióticas Interacciones con otros organismos Abióticas Interacciones con las características no biológicas como temperatura, radiación solar y cantidad de agua Movimiento Espacio disponible para dispersión Estos tres componentes se pueden representar con un diagrama de Euler de modo que las áreas ocupadas por un organiso corresponden a la intersercción de los tres conjuntos \\(B \\cap A \\cap M\\). Figura 3.2: Diagrama BAM clásico muestra cómo la geografía ocupada (la distribución realizada de una especie) resulta de la intersección de los factores bióticos, abióticos y de movimiento. En gran parte de la literatura existente los términos “Species Distrobution Models” y “Ecological Niche Models” se usan indistintamente. Es innegable que con ambos métodos se pueden obtener productos similares, sin embargo aquí vamos a hacer un distinción: Modelos de distribución Buscan estimar la geografía ocupada de una especie, la intersección \\(B \\cap A \\cap M\\). Modelos de nicho ecológico Buscan estimar la tolerancia fisiológica de una especie a las condiciones abióticas, únicamente el componente \\(A\\) de la figura 3.2. Para la gran mayoría de las especies, los nichos ecológicos sólo pueden ser estimados con experimentos bien controlados, lo cual puede ser éticamente inaceptable y logísticamente imposible. La modelación correlativa de nichos, entonces, es una de las pocas alternativas con que se cuenta para estimar los efectos del clima sobre las especies. Naturalmente el resultado de todos estos análisis tienen diferentes significados, los cuales tienen que corresponder con el nombre que se les da. Los modelos de distribución son representaciones de la distribución realizada de una especie, mientras que la representación cartográfica de los modelos de nicho son de la distribución potencial. Debido a que es muy difícil estimar las interacciones bióticas con métodos correlativos, los modelos de distribución suelen estimar relaciones más complejas entre el medio ambiente y la presencia de las especies. Los modelos de nicho ecológico, estiman relaciones más simples y generalmente con respuestas suavizadas. Figura 3.3: Ejemplo de diferencias en las respuestas de un organismo a la temperatura promedio en un modelo de distribución y un modelo de nicho ecológico. Los métodos que se utilizan para estimar las respuestas al medio ambiente en la modelación correlativa de nichos ecológicos, son en su gran mayoría modelos estadísticos. El más popular de ellos es MaxEnt, pero en este curso nos enfocaremos más en el uso de Modelos para Procesos de Puntos, los cuales pueden ser considerados como equivalente a MaxEnt, con la ventaja de que son más flexibles y permiten maximizar la utilidad de los datos tomando en cuenta la relación entre los puntos y la geografía. Una gran desventaja de los procesos de puntos es que son más laboriosos de implementar que MaxEnt pues requieren más programación en R, en particular con el paquete spatstat Baddeley and Turner (2005). 3.2 Insumos para el modelado correlativo de nichos ecológicos Como se vio anteriormente el modelado correlativo de nichos ecológicos consiste principalmente del análisis de las localidades donde se ha observado a indiviuos de una especie en relación a las condiciones ambientales de esas localidades. Para hacer estos análisis entonces se requieren principalmente dos tipos de insumos: Base de datos con las coordenadas donde se ha registrado la presencia de individuos de la especie Base de datos de las características ambientales (climáticas, vegetación topografía) de la región de estudio Para obtener las bases de datos sobre los registros de presencia de las especies se puede acceder a repositorios de acceso libre como Global Biodiversity Information Facility y Naturalista. En estos repositorios se pueden hacer búsquedas manuales para crear una base de datos que será descargada. Estas bases de datos contienen información sobre la identidad de los organismos y los individuos, si están presentes en una colección zoológica (GBIF), cuándo fueron registrados y la persona que obtuvo el registro. Naturalista, sin embargo es una colección de foto-registros hechos por ciudadanos no especialistas y que son posteriormente identificados o confirmados por algún experto/a. Los datos ambientales también pueden obtenerse de repositorios de acceso abierto. Las dos bases más populares son WorldClim y Chelsa. Ambas contienen información únicamente climática, y debido a que en este curso se enfoca en la modelación de nichos ecológicos son las que más utilizaremos. 3.2.0.1 Tipos de archivos 3.2.0.1.1 Datos de presencia Estas bases de datos son por lo general tablas con columnas y filas. Las columnas representan descripciones de los individuos registrados y cada fila corresponde a un individuo de la especie de interés. Las extenciones de los archivos que vamos a descargar son .csv (Comma Separated Value), que son archivos de texto como los .txt salvo que contienen las columnas separadas por comas y las filas por cambios de renglón (intro). Los archivos .csv pueden inspeccionarse a mano con Excel, aunque en ocasiones este puede dañar los archivos y hace que salgan errores cuando los abrimos en R. 3.2.0.1.2 Datos ambientales Estos datos están en formatos legibles por algún sistema de información geográfica (como QGIS o ArcGIS). Las extensiones de estos archivos pueden ser .bil, .tif, .asc y son en esecia una representación geo-referenciada de valores de las diferentes variables (temperatura, precipitación, etc.). En ambas bases de datos (WorldClim y Chelsa), hay dos modalidades de datos, las variables por separado en promedios mensuales (temperatura mínima y máxima y precipitación), o en forma de variables bioclimáticas. Estas son 19 variables que representan algunas combinaciones de temperatura y precipitación, p. ej.: bio 1 Temperatura anual promedio bio 2 Rango anual promedio de temperatura bio 9 Temperatura promedio del cuarto más seco bio 15 Estacionalidad de la recipitación 3.2.0.2 Manejo de datos Todos los datos los vamos a manejar en R utilizando algunas paqueterías básicas como raster, rgdal y dismo. 3.3 El proceso de calibración Se llama calibración al proceso de análisis y desarrollo del modelo. Como se mencionó en la introducción a modelación de nichos, estos son mayoritariamente, modelos estadísticos. Por lo tanto en el contexto de modelación estadística, haremos un breve repaso de cómo se ajusta un modelo a los datos. Para comprender este proceso vamos a revisar el concepto matemático de función y dos tipos de estas, la línea recta y la parábola. 3.3.0.1 La función En ciencias frecuentemente se estudia cómo un fenómeno afecta a otro. Por ejemplo, cómo la disponibilidad de recurso afecta el crecimiento poblacional. En modelación de nichos ecológicos vamos a ver cómo las características ambientales afectan la presencia de una especie (en términos muy laxos). Estas relaciones pueden ser representadas matemáticamente utilizando el concepto de función. Una función es una regla de correspondencia entre dos conjuntos \\(x\\) y \\(y\\), de modo que para cada elemento del conjunto \\(x\\) existe un solo elemento del conjunto \\(y\\). Los elementos de \\(y\\) son llamados la imagen de \\(x\\) bajo la regla de correspondencia \\(f\\), que se denota como \\(f(x)\\). La función \\(f(x)\\) se lee \\(f\\) de \\(x\\), y el conjunto \\(x\\) es el dominio y \\(y\\) es el codominio de \\(f\\). Debido a que todos los valores de \\(y\\) son producidos por la función \\(f\\), a \\(x\\) también se le conoce como variable independiente y a \\(y\\) como dependiente (puesto que depende de \\(x\\)). En términos prácticos \\(f(x) = y(x)\\). La manera en que los valores de \\(y\\) corresponden a ciertos valores de \\(x\\) está determinado por unas reglas concretas, generalmente en forma de una operación ó serie de operaciones matemáticas. Las siguientes son ejemplos de funciones \\(y(x) = a + bx\\) \\(y(x) = x^2\\) \\(f(x) = \\sin(x - c)\\) \\(f(x) = e^{rx}\\) \\(f(x) = a \\exp(b - cx)\\) En todas estas funciones la variable independiente es \\(x\\) y la variable cuyos valores dependen de \\(x\\) son \\(y(x)\\) ó \\(f(x)\\). A continuación veremos dos tipos de funciones que utilizaremos ampliamente durante este curso para modelar la relación entre la presencia de las especies y las características ambientales. 3.3.0.2 La línea recta La recta es un tipo de función que cuando la graficamos en el plano cartesiano describe una línea recta. Matemáticamente la recta se representa de la siguiente manera: \\[ y(x) = a + bx\\] Para que se cumpla que \\(y(x)\\) sea geométricamente una línea recta, la variable \\(x\\) sólo puede ser multiplicada por una constante, que llamaremos \\(b\\) de manera general, y sumada otra constante que llamaremos \\(a\\). Estas dos constantes afectan la apariencia de la recta, cambiando el valor de \\(y\\) cuando \\(x = 0\\), y su inclinación. Figura 3.4: Ejemplos de funciones lineales con las ecuaciones que las generan. 3.3.0.3 La Parábola La parábola es una función que contiene un término cuadrático en \\(x\\). La forma más simple de una parábola es: \\[ y(x) = x^2 \\] Esta función, al igual que la recta, se puede modifcar anadiendo más términos de \\(x\\), sí y solo sí los exponentes sean \\(&lt; 2\\), y tambiénse pueden añadir constantes. De tal modo que la forma mś general de una parábola es: \\[ y(x) = a + bx +cx^2 \\] Las constantes \\(a\\), \\(b\\) y \\(c\\) pueden tomar cualquier valor \\(\\in \\mathbb{R}\\) que se encuentre en el conjunto de los números reales. Al igual que ocurre con la recta, las constantes \\(a\\), \\(b\\) y \\(c\\) modifican la apariencia de la parábola. Como resulta evidente, cuando \\(c = 0\\), la ecuación de la parábola dibuja en una línea recta. A continuación se muestran gráficas de parábolas con diferent valore de \\(a\\), \\(b\\) y \\(c\\). 3.3.1 Tutorial de R R Es el lenguaje de programación estadística más popular, incluso como paquetería supera con mucho a aplicaciones comerciales como SPSS o SAS. Como lenguaje de programación tiene sus peculiaridades sintácticas. En R básico hay muchas funciones nativas que pueden hacer procesos complejos o manejar diferentes tipos de objetos. Algunos de los tipos de objetos más comunes y las funciones que los crean son: c() Concatena los valores numéricos o caracteres que se pongan al interior de los paréntesis separados por comas: c(1, 2, 3, 4) data.frame() Se utiliza para crear tablas cuyas columnas son diferentes variables que describen los objetos representados en filas. Los argumentos que contiene son los nombres de las columnas y sus contenidos: data.frame(x = c(1, 2, 3, 4), y = c(4, 3, 2, 1), nombre = c(\"a\", \"b\", \"c\", \"d\")) Los objetos creados con estas funciones tienen que ser almacenados en la memoria de la sesión de R que se está ejecutando para poder hacer operaciones. La asignación a un objeto se puede hacer con: x &lt;- c(1, 2, 3, 4) Una vez creado el objeto x podemos verificar sus contenidos escribiendo x en la consola de R: x ## [1] 1 2 3 4 En los análisis estadísticos se emplean las bases de datos en forma de data.frame. Estos no tienen que construirse a mano en R, de hecho se pueden generar en una aplicación externa como Excel e importarlos al espacio de trabajo de R con la función read.csv(). Los argumentos que se ponen dentro de () son la ruta y nombre del archivo que queremos leer, por ejemplo \"Documentos/Tarea-R/Tabla.csv\", le indica a R dirigirse a la carpeta \"Documentos\" subcarpeta \"Tarea-R\", archivo \"Tabla.csv\". Para evitar complicaciones casi todas las tablas o bases de datos que leeremos en R serán de tipo .csv. Para acceder a los valores almacenados en los objetos data.frame utilizamos el operador $ de la siquiente manera: tabla &lt;- data.frame(x = c(1, 2, 3, 4), y = c(4, 3, 2, 1), nombre = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) tabla ## x y nombre ## 1 1 4 a ## 2 2 3 b ## 3 3 2 c ## 4 4 1 d y para acceder a los valores en la columna con nombre x. tabla$x ## [1] 1 2 3 4 3.3.1.1 Procedimientos estadísticos básicos Todas las medidas descriptivas de distribuciones estadísticas están implementadas en R baśico bajo las siguientes funciones: mean(x) Media aritmética (promedio) del objeto numérico x median(x) Mediana del objeto numérico x sd(x) Desviación estándar del objeto numérico x range(x) Rango de x (diferencia entre valor máximo y mínimo) min(x), max(x) Valores mínimos y máximos de x respectivamente summary(x) Resumen descriptivo de x (todas las anteriores en un solo comando) Todas estas funciones requieren que el objeto x contenga valores numéricos (que x sea contínua o discreta). 3.3.1.2 Análisis de regresión En regresión lineal simple, utilizamos R para estimar los coeficientes \\(a\\) y \\(b\\) de una ecuación lineal como \\[ y = a + bx \\] Cuando una variable independiente \\(y\\) es afectada por más de una variable \\(x\\), la regresión lineal se utiliza para estimar todos los coeficientes \\(a\\) (intercepto) y \\(b_i\\): \\[ y=a + b_1 x_1 + b_2 x_2 + \\dots + b_n x_n \\] Función nativa de R para hacer regresión lineal es lm (linear model). Y los argumentos que se necesitan son: La fórmula del modelo La base de datos que contiene la información de \\(y\\) y \\(x\\) (estas pueden tomar cualquier nombre) 3.3.1.2.1 La fórmula del modelo La sintaxis para especificar el modelo es: y~x que es equivalente a \\[ y = a + bx \\] De modo que R estimará ambos coeficientes. Si fuer necesario indicarle a R que el intercepto \\(a\\) no debe ser estimado se hace de la siguiente manera: y~ -1 + x Para los casos en que existe más de una variable independiente éstas sólo tienen que ser añadidas con +: y ~ x1 + x2 + x3 + x4 ... 3.3.1.2.2 Especificación de la base de datos Cualquier llamado a la función lm debe hacerse con objetos existentes en el espacio de trabajo de R, y los nombres que se incluyan en la fórmula deben coincidir. Estos objetos pueden estar contenidos en las columnas de un data.frame. Para entender esto veamos el llamado: lm(y ~ x, data = tabla) Como es evidente y ~ x corresponde a la fórmula, mientras que el argumento data = tabla le indica a R que los objetos x y y están contenidos en sus columnas. 3.3.1.3 Importación de datos a R Para importar una base de datos al espacio de trabajo de R necesitamos contar con el archivo y conocer su ubicación en el disco duro de la computadora. Por defecto, R buscará los archivos en la carpeta home del usuario. Es posible indicarle a R que en esa sesión utilice otra ubicación, de modo que la búsqueda e importación de datos sea más conveniente. La función que nos permite conocer la carpeta donde R buscará los archivos es: getwd() ## [1] &quot;/home/gerardo/Documentos/Cosas ENES/Materias/Posgrado/Modelacion-nichos-distr-PPMs/Unidad-I&quot; y produce una cadena de texto que es la dirección dentro del disco duro. Para modificarla, podemos usar la función setwd() donde debemos indicar con una cadena de texto la ruta a seguir a partir de la ubicación actual: setwd(&quot;~/Documentos/SubCarpeta&quot;) Una vez que se ha especificado la ruta donde los archivos a importar están ubicados, podemos tratar de leerlos con la función read.csv(), si es que los datos a importar están en ese formato. El argumento para read.csv es el nombre del archivo que se busca rodeado por comillas: setwd(&quot;~/Documentos/SubCarpeta&quot;) tabla &lt;- read.csv(&quot;tabla.csv&quot;) En ocasiones es necesario especificar el caracter que separa las columnas del archivo csv. Por defecto R buscará comas (,), pero también son comunes los tabuladores (datos de GBIF), en cuyo caso se debe indicar con el argumento sep = \"\\t\" (separation = tabulador). El nombre del caracter que separa es el valor del argumento sep. 3.3.1.4 Ajuste de un modelo de regresión Una vez importada la base de datos al espacio de trabajo de R, podemos continuar con el ajuste. El análisis que se muestra a continuación está basado en unos datos simulados. Los detalles de la simulación los puedes ver aquí, y la base de datos simulada aquí. Comenzaremos por leer la base de datos con read.csv y asignarla a un objeto llamado datos: datos &lt;- read.csv(&quot;Base-ejemplo-reg.csv&quot;) head(datos) ## x y ## 1 23.33946 1.8622077 ## 2 11.57775 1.3431069 ## 3 18.97340 2.4266392 ## 4 13.85936 0.5492634 ## 5 20.33512 2.3912786 ## 6 27.58100 2.7274168 La función head imprime las primeras seis filas de la tabla que se ponga como argumento. Podemos ver entonces que la base consta de dos filas llamadas x y y. Como no tenemos más detalles experimentales que los nombres de las variables asumimos que y es producida por xy será por lo tanto la variable de respuesta. Entonces para ajustar el modelo lineal utilizamos lm y el resultado lo almacenamos en el objeto modelo1: modelo1 &lt;- lm(y ~ x, data = datos) Para revisar los coeficientes estimados, podemos utilizar la función summary: summary(modelo1) ## ## Call: ## lm(formula = y ~ x, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.41492 -0.41252 0.06171 0.39278 1.32792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.008346 0.203559 -0.041 0.967 ## x 0.102373 0.009846 10.397 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5499 on 98 degrees of freedom ## Multiple R-squared: 0.5245, Adjusted R-squared: 0.5196 ## F-statistic: 108.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 La información relevante al ajuste, la encontramos en la tabla Coefficients. La primera fila, con nombre Intercept contiene el valor promedio del intercepto (parámetro \\(a\\) en \\(y = a + bx\\)) en la columna Estimate y el error estándar en la columna Std. Error. La probabilidad de que el valor estimado sea cero se encuentra en la columna Pr(&gt;|t|). Si un coeficiente estimado tiene una alta probabilidad de ser cero, se dice que es no significativo. En este caso, el intercepto es probablemente \\(a \\approx 0\\). En relación al parámetro \\(b\\) de la segunda fila, tiene una probabilidad muy baja de ser cero. Un parámetro estimado en regresión lineal simple es \\(R^2\\), que indica qué tan cerca están los datos de la línea de regresión. Para procedimientos como los que veremos en el resto del curso este parámetro no tiene mayor relevancia. En este caso \\(R^2 = 0.51\\), el cual es un valor intermedio (\\(0\\leq R^2\\leq1\\)). Para ver los datos y la línea de regresión podemos representarlos en el plano cartesiano con la función plot: plot(datos$x, datos$y, xlab = &quot;x&quot;, ylab = &quot;y&quot;) x.modelo &lt;- c(min(datos$x), max(datos$x)) y.modelo &lt;- predict(modelo1, newdata = data.frame(x = x.modelo)) lines(x.modelo, y.modelo, col = &quot;red&quot;) Si llegaramos a contar con más variables x simplemente tendríamos que añadirlas a la fórmula del modelo cuando hacemos el llamado a lm. Un problema un poco más difícil que resolver es la decisión de qué modelo vamos a seleccionar cuando tenemos muchas variables, pues hay variables independientes que no son compatibles, como aquellas que están correlacionadas. Existen metodologías o mediciones de la bondad de ajuste del modelo a los datos como el criterio de información de Akaike (AIC) que penaliza los modelos complejos en relación a aquellos más sencillos. Estas metodologías de selección de modelo las revisaremos más adelante. 3.3.2 Calibración de modelos de nicho ecológico En modelación estadística, calibración es sinónimo de ajuste, por lo tanto el procedimiento de estimar los coeficientes \\(a\\) y \\(b\\) para la relación entre \\(x\\) y \\(y\\) es equivalente a la calibración. En un ejercicio más realista, sin embargo, nos veremos en la necesidad de ajustar varios modelos y después seleccionar el que mejor explica los datos observados. En regresión lineal simple se utiliza el parámetro \\(R^2\\). Este tienen muchos bemoles como el ignorar la complejidad del modelo. Con complejidad, nor referimos a la cantidad de co-variables \\(x\\) y la cantidad de parámetros \\(b\\). Siempre es buena idea seleccionar modelos simples, para lo que se utiliza el Criterio de Información de Akaike. Este criterio es una medida de ajuste y complejidad de modelo, y sirve para comparar modelos diferentes para el mismo conjunto de datos. Cuando utilizamos el AIC se busca minimizarlo, es decir, que la varianza residual es pequeña y que el número de parámetros también es lo más pequeño posible. No obstante, el uso del AIC en modelación correlativa de nichos ecológicos es marginal. Generalmente se busca optimizar (maximizar) alguna medida de desempeño predictivo. En este curso, nos enfocaremos en seleccionar modelos utilizando el AIC y la verifiación de la estimación correcta de los efectos estadísticos, aunque no ignoraremos la importancia del análisis de la capacidad predictiva. Existen muchas medidas diferentes del desempeño predictivo, pero la más adecuada para los modelos que desarrollaremos es el análisis de curvas ROC parciales (Peterson2008?). De manera muy general, el proceso de calibración de modelos de nicho ecológico consiste de: Selección de variables ambientales Propuesta de modelos alternativos Selección de método de análisis Formateo de datos Ajuste de modelos alternativos Selección de modelo Estimación de capacidad predictiva 3.4 Uso de R como Sistema de Información Geográfica Para ajustar modelos espaciales es necesario hacer mucho manejo de datos geográficos. Afortunadamente R cuenta con suficientes paquetes para hacerlo. Para este propósito Aquí usaremos los paquetes raster y rgdal con algunos otros como auxiliares. 3.4.1 Lectura de capas raster Para crear objetos raster en el espacio de trabajo de R vamos a utilizar la función rasterdel paquete del mismo nombre. Para usarla, el único argumento que necesitamos es el nombre y ubicación del archivo, en este caso la ubicación es la carpeta “Datos-ejemplos” y el nombre del archivo es “Var-1.tif” library(raster) r &lt;- raster(&quot;../Datos-ejemplos/Var-1.tif&quot;) Para visualizar la capa importada podemos usar la función plot: plot(r) En un análisis realista, uno utiliza múltiples capas, para lo cual existe la función stack, lo que permite crear un objeto con múltiples capas. Primero necesitamos conocer los nombres de los archivos a importar con la función list.files, y el objeto que contiene los nombres de los archivos lo utilizamos de argumento para stack archivos &lt;- list.files(&quot;../Datos-ejemplos&quot;, &quot;tif&quot;, full.names = T) archivos ## [1] &quot;../Datos-ejemplos/Var-1.tif&quot; &quot;../Datos-ejemplos/Var-2.tif&quot; ## [3] &quot;../Datos-ejemplos/Var-3.tif&quot; s &lt;- stack(archivos) Y podemos visualizar de nuevo con plot plot(s) Para que la función stack funcione todas las capas tienen que coincidir plenamente píxel por píxel. Si tenemos capas que no coinciden será necesario remuestrearlas con la función resample. Esta función necesita además conocer el sistema de coordenadas en que se encuentra la capa. El sistema de coordenadas más común es WGS84, y su código EPSG:4326. Este código es un sistema internacional que permite conocer los dieferentes sistemas de coordenadas para las diferentes regiones del mundo. En R podemos verificar qué sistema de coordenadas tiene una capa con la función proj4string: proj4string(r) ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; Si la capa tiene declarado el sistema de coordenadas no es necesario indicarselo a R y las capas se pueden sincronizar: r1 &lt;- raster(&quot;../Datos-ejemplos/Var-1.tif&quot;) r2 &lt;- raster(&quot;../Datos-ejemplos/Var-2.tif&quot;) r1.1 &lt;- resample(r1, r2) Este proceso tiene que ser repetido para todas las capas que queremos incluir en un stack. Otra operación común es el recorte de capas para reducir el tamaño. Esto es necesario tanto por razones biológicas como computacionales. Biológicamente no tiene sentido tratar de modelar la favorabilidad de ambientes que una especie no ha experimentado. Computacionalmente puede ahorrarnos muchos recursos y tiempo si seleccionamos un área de estudio de un tamaño adecuado. Como regla de dedo debemos excluir todas las áreas que no son accesibles para una especie. La función que se utiliza para recortar una capa raster es mask del paquete raster. El primer argumento que necesitamos es la capa o capas raster a recortar y la capa vectorial que delimita las zonas de estudio. Comenzaremos por importar los datos de presencia con la función read.csv puntos &lt;- read.csv(&quot;../Datos-ejemplos/Puntos-tutorial-2.csv&quot;) Veamos cómo están distribuidos sobre el área de estudio: plot(r) points(df) Para delimitar el área de estudio, sueles utilizare buffers alrededor de los puntos de presencia, que pueden ser generados con la función gBuffer del paquete rgeos. Para generarlos tenemos que indicarle a R cuál columna corresponde a las coordenadas latitud y longitud: coordinates(df) &lt;- ~ x+y buffers &lt;- rgeos::gBuffer(df, width = 0.3) Y para verificar los buffers generados: plot(r) points(df) plot(buffers, add = T) Una vez obtenidos los buffers podemos usarlos para recortar las capas raster que funcionarán como covariables ambientales. s.rec &lt;- mask(s, buffers) plot(s.rec) References "],["unidad-ii-modelación-de-nichos-ecológicos-con-procesos-de-puntos.html", " 4 Unidad II: Modelación de nichos ecológicos con procesos de puntos 4.1 Introducción 4.2 Formateo de datos", " 4 Unidad II: Modelación de nichos ecológicos con procesos de puntos 4.1 Introducción Cuando analizamos datos con alguna metodología de regresión siempre tenemos claro cuál es la variable de respuesta. Por ejemplo, en un análisis de temperaturas como función de la elevación sobre el nivel del mar y la latitud, el modelo ajustado arrojará valores en las unidades \\(°\\mathrm{C}\\), o las que hayamos utilizado. Aunque la modelación correlativa de nichos esté en gran medida basada en la modelación estadística, ha prevalecido una desconexión sustancial entre lo que se modela y lo que los modelos estadísticos arrojan. La herramienta estadística de regresión que resuelve en buena medida esta desconexión son los Modelos de Procesos de Puntos (MPPs). En MPPs, la variable analizada es la intensidad de puntos, por lo que un MPP predice intensidad. Los datos de sólo presencia de especies pueden ser concebidos de manera más general como un proceso de puntos (la colección de coordenadas a modelar) sobre una rejilla de unidades espaciales de tamaño fijo. Con esto en mente, la intesidad se define como el número promedio de puntos por unidad espacial. Existe una variedad de MPPs, pero el que estudiaremos aquí son los MPPs Poisson. En estos, la variable intensidad es modelada como una función log-lineal de la distribución Poisson (para conteos), de un conjunto de covariables ambientales. Por lo tanto, los MPPs Poisson son muy similares a los modelos lineales generalizados. Estos métodos tienen ya una larga trayectoria en estadística espacial y geoestadística, para el análisis de variables aleatorias definidas en el espacio. Muchos de estos métodos existen incluso antes que algunos métodos utilizados rutinariamente para la generación de modelos de nicho ecológico o áreas de distribución. De hecho, un análisis matemático de 2013, encontró que Maxent es en esencia un MPP Poisson. Para aprender a analizar conjuntos de puntos de ocurrencia utilizaremos el programa R y el paquete spatstat, principalmente. En esta unidad introductoria, aprenderemos a formatear los datos de ocurrencia y las variables ambientales para que puedan ser utilizadas por spatstat. Posteriormente veremos cómo se hace un análisis exploratorio para identificar las variables que potencialmente explican los patrones de intensidad de puntos de nuestra base de datos. Finalmente, veremos cómo se ajusta, selecciona y diagnostica estadísticamente un MPP Poisson. Cabe mencionar que aquí nos enfocaremos en el aspecto técnico-metodológico, aunque los conceptos como área de accesibilidad, modelo de nicho vs. modelo de distribución siguen aplicandose como en cualquier otra herramienta para el análisis de nichos ecológicos y distribuciones geográficas. 4.2 Formateo de datos Los formatos básicos en que podemos tener almacenados los datos son los tradicionales .csv, para las ocurrencias, y .tif para los raster. La razón principal por la que se necesita un formato especial para los datos es que spatstat utiliza objetos de clase im o imágenes en lugar de raster, un patrón plano de puntos, objetos de clase ppp para los datos de ocurrencia, que normalmente se manejan como un data.frame, y una ventana de trabajo que es obtiene a partir de los raster. Hacer la transformación entre formatos es esencial pues spatstat cuenta con muchas funciones para hacer del análisis de datos en estos formatos bastante sencillo. 4.2.1 Formateo de datos raster He escrito una función de R para transformar de raster a im. La transformación de un data.frame a ppp es bastante sencilla y no requiere de una función especial. Entonces, para comenzar necesitamos tener instalados los paquetes raster, rgdal, spatstat y foreach, lo cual se consigue corriendo el siguiente código en la consola de R: install.packages(c(\"raster\", \"rgdal\", \"spatstat\", \"foreach\")) Después de la instalación, podemos usarlos para cargar una sola capas raster al espacio de trabajo con la función raster, contenida en el paquete del mismo nombre: library(raster); library(spatstat) r &lt;- raster(&quot;../Datos-ejemplos/Var-1.tif&quot;) El único argumento que se pasa a la función raster es la ruta y nombre del archivo. En este caso el archivo está en la carpeta \"Datos-ejemplos\", y el archivo se llama Var-1.tif. Es importante siempre incluir la extensión del archivo (las 2-4 letras después del punto). En caso de necesitar cargar más de un archivo al mismo tiempo, podemos utilizar la función stack. Aquí es importante señalar que para que esta función pueda cargar los archivos, estos tienen que estar perfectamente alineados y tener exactamente la misma extensión espacial. El argumento que tenemos que pasar a la función stack es la lista de archivos a leer. Esta podemos generarla automáticamente con la función list.files, cuyos argumentos son la carpeta de búsqueda, extensión de los archivos a listar, y si necesitamos la ruta completa junto con los nombres de los archivos: arch &lt;- list.files(&quot;../Datos-ejemplos/&quot;, &quot;.tif&quot;, full.names = T) s &lt;- stack(arch) Una vez cargados las capas, podemos graficarlas para verificar que sean las que necesitamos o tenemos en mente: plot(s) Figura 4.1: Gráfica de las capas que usaremos para los ejemplos. La transformación a im la haremos solo con el stack, pues es el escenario más probable al que se encontrarán (trabajo con varias capas). Comenzaremos cargando la función para hacer la transformación, que se llama imFromStack. Para hacerlo utilizaremos la función source, y el argumento que necesita es la ruta y nombre del archivo de texto que contiene la función: source(&quot;../Funciones-spatstat/imFromStack.R&quot;) Una vez cargada, podemos utilizarla, pasando como únigo argumento el nombre del objeto en el espacio de trabajo de R que contiene las capas: s.im &lt;- imFromStack(s) Para verificar el tipo de objeto que resulta, podemos correr la función: class(s.im) ## [1] &quot;list&quot; Como podemos ver es una lista, y cada uno de sus elementos es una imagen tipo im: class(s.im[[1]]) ## [1] &quot;im&quot; Podemos hacer la gráfica, aunque ahora tenemos que hacerlo una por una: par(mfrow = c(2, 2)) plot(s.im[[1]]) plot(s.im[[2]]) plot(s.im[[3]]) Puedes descargar la función imFromStack en esta liga. 4.2.2 Obteniendo la ventana de trabajo Antes de continuar, considero importante establecer que el método de obtención de la ventana de trabajo que he implementado asume que el área de accesibilidad de la especie de análisis es toda la extensión del raster que se utilice para obtener la ventana del área de trabajo. La función para obtener la ventana de trabajo a partir de un raster es winFromRaster (disponible aquí), y la importamos del mismo modo que imFromStack: source(&quot;../Funciones-spatstat/winFromRaster.R&quot;) win &lt;- winFromRaster(s) class(win) ## [1] &quot;owin&quot; 4.2.3 Formateando los registros de ocurrencia Primero, necesitamos contar con una base de datos de ocurrencia, en esta ocasión por tratarse de una un ejemplo de formateo, simularé una con la función randomPoints del paquete dismo, y solo para hacerla un poco más interesante le voy a añadir ruido con una distribución normal a las coordenadas \\(x, y\\): puntos &lt;- data.frame(dismo::randomPoints(s, 200)) puntos$x &lt;- puntos$x + rnorm(200, 0, 0.05) puntos$y &lt;- puntos$y + rnorm(200, 0, 0.05) Ahora bien, para transformar el objeto puntos de un data.frame a ppp utilizaremos pa función del mismo nombre, y la ventana de trabajo que creamos arriba: puntos.ppp &lt;- ppp(x = puntos$x, y = puntos$y, window = win, check = F) class(puntos.ppp) ## [1] &quot;ppp&quot; En el primer argumento x se especifican las coordenadas \\(x\\) o longitud, en y las coordenadas \\(y\\) o latitud, en window la ventana de trabajo, y check especifica si se verificará que todos los puntos están dentro de la ventana de trabajo. Normalmente se deja en check = T. Con todos los datos formateados, entonces ahora graficamos la ventana de trabajo y el proceso de puntos: plot(win); points(puntos.ppp, col = &quot;white&quot;) "],["unidad-ii---utilizando-spatstat.html", " 5 Unidad II - Utilizando spatstat 5.1 Análisis exploratorio 5.2 Ajustando un proceso de puntos", " 5 Unidad II - Utilizando spatstat 5.1 Análisis exploratorio Antes de proponer un modelo para los datos de ocurrencia, es buena práctica realizar un análisis exploratorio. En realidad, el modelo que propongamos dependerá en buena medida de este análisis exploratorio. Los análisis exploratorios que yo hago comprenden una serie de pasos: Probar si los datos de ocurrencia cumplen son independientes unos de otros o si están autocorrelacionados Ver la respuesta de la intensidad de puntos ante las diferentes variables ambientales que queremos incorporar en el modelo Medir la correlación entre las diferentes variables Con base en los puntos 2 y 3 proponer un conjunto de modelos alternativos (más sobre este punto abajo) 5.1.1 Independencia y autocorrelación Se dice que los diferentes puntos son independientes entre sí, si el número de vecinos promedio de cada punto como función de la distancia, sigue una distribución Poisson (figura 5.1). Con esto en mente hay varios escenarios posibles, que en promedio cada punto tenga más vecinos de lo esperado o que tenga menos. En el primer caso, se dice que los puntos están agregados, pues un punto tiende a atraer a otros. En el segundo, los puntos están segregados, o sea que un punto tiende a mantener a otros puntos lejos de sí. El primer caso, de independencia, suele ocurrir cuando los puntos están distribuidos aleatoriamente (figura 5.2). Figura 5.1: Número de vecinos como función de la distancia en un proceso de puntos. Figura 5.2: Ejemplo de procesos de puntos de izquierda a derecha: segregado, aleatorio y agregado (reproducido de Baddeley y Rubak 2016) Existe una serie de pruebas gráficas y estadísticas para medir autocorrelación. Aquí nos enfocaremos en el uso de la prueba de envolturas K de Ripley. La implementación de esta prueba en spatstat genera unos intervalos de confianza alrededor de la expectativa del número de vecinos por medio de simulación. La figura 5.3 muestra los tres escenarios de segregación, aleatorio y agrecación. Figura 5.3: Gráfica de la prueba K de Ripley implementada en spatstat. De izquierda a derecha: funciones de Ripley para puntos segregados, aleatorios y agregados. 5.1.1.1 Análisis de autocorrelación en R con spatstat Haremos este análisis de autocorrelación con el mismo proceso de puntos que formateamos anteriormente. La función de spatstat para la prueba de Ripley es envelope, y los argumentos que requiere son 1) el proceso de puntos a analizar, 2) la función con que se medirá autocorrelación (Kest para \\(K\\) de Ripley) y 3) el número de simulaciones. Normalmente, para un nivel de significancia \\(P=0.05\\), se utilizan 39 simulaciones, el cual deberá aumentar si el umbral de significancia buscado es más estricto (\\(P = 0.01\\), p. ej.). K &lt;- envelope(puntos.ppp, fun = Kest, nsim = 39) ## Generating 39 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39. ## ## Done. Mientras la función corre, R imprime la última simulación completada, y el objeto que se obtiene puede graficarse con el método por defecto plot: plot(K) Figura 5.4: Gráfica de la función K de Ripley para el proceso de puntos analizado. Las sombras en gris muestran los intervalos de confianza al 95%. La línea roja a guiones representa la expectativa teórica (K teórica) en caso de que el proceso de puntos sea aleatorio, y la línea negra sólida es la función de Ripley para el proceso de puntos (la K observada). El eje de las x representa distancia (en grados) y las y el número promedio de vecinos de cada punto. 5.1.2 Análisis gráfico de las respuestas al medio ambiente. Para este análisis simularé una base de datos de ocurrencia donde la probabilidad de observarlos sea inversamente proporcional a la distancia de una centroide pre definido. Con base en ello, podremos ver cómo cambia la intensidad de puntos en realción a los diferentes valores de cada variable. 5.1.2.1 Simulación de datos de presencia Utilizaré las mismas variables que para el ejercicio anterior de formateo, y el centroide estará localizado en la media aritmética de cada capa: centroide &lt;- cellStats(s, mean) Para calcular la distancia al centroide, necesitamos la covarianza entre las diferentes capas, de modo que la calculamos con la función cov. Hay implementaciones más robustas en el paquete MASS, para nuestros propósitos pedagógicos cov es suficiente. Comenzamos entonces, transformando el stack en una tabla: s.df &lt;- data.frame(rasterToPoints(s)) covar &lt;- cov(s.df[, 3:5]) Posteriormente, utilizando el centroide y la matriz de covarianza, generamos las distancias utilizando las tres columnas del objeto s.df que contienen los valores de las variables ambientales: md &lt;- mahalanobis(s.df[, 3:5], center = centroide, cov = covar) Y transformamos las distancias al centroide en una capa raster: md.r &lt;- rasterFromXYZ(data.frame(s.df[, 1:2], md)) plot(md.r) Figura 5.5: Distancia Mahalanobis al centroide de las capas. Para simular las ocurrencias, transformaré la capa de distancias exponencialmente, para obtener una superficie probabilística: md.exp &lt;- exp(-0.5*md.r) plot(md.exp) Figura 5.6: Distancia Mahalanobis transformada exponencialmente para simular presencias. Verde indica mayor probabilidad de ocurrencia. Para simular las presencias usaré el mismo método que anteriormente, pero en esta ocasión la probabilidad determinará las celdas en que habrá puntos: set.seed(182) puntos.2 &lt;- dismo::randomPoints(mask = md.exp, n = 200, prob = T) ## Warning in .couldBeLonLat(x, warnings = warnings): CRS is NA. Assuming it is ## longitude/latitude puntos.2 &lt;- data.frame(puntos.2) puntos.2$x &lt;- puntos.2$x + rnorm(200, 0, 0.05) puntos.2$y &lt;- puntos.2$y + rnorm(200, 0, 0.05) plot(md.exp); points(puntos.2) #### Graficación y análisis de las respuestas Para continuar con el análisis, necesitamos formatear el objeto puntos.2 como ppp: puntos.2.ppp &lt;- ppp(x = puntos.2$x, y = puntos.2$y, window = win, check = F) Recordemos que el objeto win lo generamos en la sección de formateo de este tutorial. Para el análisis de las respuestas necesitamos crear otro objeto, que contiene los conteos de cuadratura, es decir, cuántas presencias por unidad espacial, con la función pixelquad que requiere de dos argumentos, el proceso planar de puntos y la ventana de trabajo en formato owin: Q &lt;- pixelquad(X = puntos.2.ppp, W = as.owin(win)) Dado que las capas ya están también formateadas como im, podemos ahora sí continuar con el análisis de las respuestas con la función plotQuantIntens. Esta función generará unos gráficos en pdf que deberemos revisar después de correrla. Para cargar la función, haremos igual que con las anteriores. Puedes descargar la función aquí. Esta función requiere de varios argumentos: La lista de imágenes que se usarán para ver cómo cambia la intensidad de puntos en relación a cada variable El número de cuantiles en que se cortará cada variable para representar la intensidad en el espacio El objeto de cuadratura (Q) El objeto con los puntos en formato ppp El nombre del directorio donde se guardará el archivo pdf El nombre del archivo source(&quot;../Funciones-spatstat/plotQuantIntens.R&quot;) plotQuantIntens(imList = s.im, noCuts = 5, Quad = Q, p.pp = puntos.2.ppp, dir = &quot;&quot;, name = &quot;Responses-centroid&quot;) ## png ## 2 El archivo de gráficas que produce plotQuantIntens muestra en cada panel: La variable analizada con el número de puntos en cada región de valores especificada por el argumento noCuts o número de cortes La variable analizada con el proceso de puntos sobrepuesto La respuesta de la intensidad de puntos a la variable analizada. En el eje de las \\(x\\) (horizontal), el valor de la variable, y en el eje de las \\(y\\) (vertical) la intensidad (número) de puntos. La idea de este análisis es que podamos identificar a priori qué variables podemos incluir en el modelo y con qué tipo de relación. Por ejemplo, las variables 2 y 3 tienen respuestas claramente con forma de parábola invertida o de campana con uno que otro “tope”, por lo que podemos utilizar una fórmula polinomial de \\(2^o\\) grado (figura 5.7). Para la variable 1, sin embargo parece haber una región hacia el extremo derecho del eje \\(x\\) en la cual la intensidad vuelve a incrementar. Dado la intensidad de puntos tiende a aumentar nuevamente podríamos utilizar un término cúbico, pues una ecuación cúbica puede adquirir esta forma (figura 5.8). curve(exp(1 + x - x^2), from = -3, 3) Figura 5.7: Ecuación polinomial de 2o grado exponenciada. Por otro lado, una ecuación polinomial de \\(3^{er}\\) grado: curve(exp(1+ x - 2*x^2 + x^3), from = -1.5, to = 1.4 ) Figura 5.8: Ecuación polinomial de 3er grado exponenciada. 5.1.3 Midiendo la correlación entre variables independientes Cuando hacemos un análisis de regresión, en deseable que todas las variables independientes que incluyamos en un modelo sean ortogonales, es decir que no estén correlacionadas, que no sean predictoras una de otra. Al incluir variables independientes correlacionadas creamos un problema en el que no es posible medir la varianzade la variable dependiente que explican. Un ejemplo análogo en ANOVA, sería un experimento de dos factores con dos niveles cada uno, y por lo tanto para poder entender el efecto de cada factor con sus niveles necesitaríamos cuatro unidades experimentales como mínimo, 2 niveles del factor I \\(\\times\\) 2 niveles del factor II. Si los factores fueran colineales, implicaría que sólo tendríamos por ejemplo, nivel \\(A\\) del factor I con nivel \\(a\\) del factor II, y nivel \\(b\\) del factor I con nivel \\(b\\) del factor II, siendo que requerimos de las combinaciones:\\(Aa, Ab, Ba\\) y \\(Bb\\). Para medir la correlación entre pares de variables raster estimamos el coeficiente de correlación de Pearson, con la función pairs del paquete raster. Esta función compara todas las posibles combinaciones de pares de variables en el stack y produce un gráfico de dispersión para cada combinación: pairs(s) ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter ## Warning in graphics::par(usr): argument 1 does not name a graphical parameter Figura 5.9: Prueba de correlación para todos los pares de variables. Como podemos ver, de las variables propuestas sólo podemos utilizar dos de ellas en el mismo modelo, puesto que Var.1y Var.2 están correlacionadas. La correlación entre ellas es lineal, como puede verse en el gráfico de dispersión (1a columna 2a fila). No hay una regla que indique qué coeficiente de correlación es aceptable para incluir en un mismo modelo, sin embargo, mientras más cercano a \\(0\\) y lejos de \\(1\\) ó \\(-1\\) es mejor. Ciertamente \\(0.76\\), puede ser considerada como alta correlación. Como regla general personal, si el número de variables con respuestas claras observadas con plotQuantIntens alto, podemos utilizar todas aquellas con \\(r &lt; 0.5\\), si el número de variables es pequeño podemos permitirnos un poco más de libertad e incluir en un mismo modelo todas aquellas con \\(r \\leq 0.7\\). 5.1.4 Propuesta de modelos alternativos Anteriormente vimos que la intensidad de puntos (figura ??) tiene una clara res puesta de campana en relación a Var.2 y Var.3, y posiblemente Var.1. Por ello podemos proponer una serie de funciones polinomiales de 2o grado donde sólo estén presentes Var.1 y Var.3, y Var.2 y Var.3. Recordemos que con base en el análisis de correlación no debemos incluir a Var.1 y Var.2 en el mismo modelo (figura 5.9). De modo que, en sintaxis de R, las fórmulas propuestas del modelo (incluyendo una de 3er grado para Var.1): ~ Var.1 + Var.3 + I(Var.1^2) + I(Var.3^2) ~ Var.1 + Var.3 + I(Var.1^2) + I(Var.1^3) + I(Var.3^2) ~ Var.2 + Var.3 + I(Var.2^2) + I(Var.3^2) La decisión de cuál de las fórmulas propuestas será la final con que trabajaremos, es material del próximo capítulo, por el momento veremos cómo se ajusta un ppm. 5.2 Ajustando un proceso de puntos Aquí veremos cómo se ajusta un proceso de puntos. Como vimos en el capítulo anterior, los datos que analizamos consisten de la intensidad de puntos por unidad espacial, como función de un conjunto de predictores. Los modelos que ajustaremos son aquellos que propusimos como producto del análisis exploratorio del capítulo anterior. Para ajustar un proceso de puntos Poisson, utilizamos la función ppm (por “point process model”) del paquete spatstat. Los argumentos que debemos incluir al llamar a la función son: El objeto que contiene el proceso de puntos trend que corresponde a la fórmula del modelo La lista de imágenes de pixeles que contiene las covariables que se utilizan en la fórmula (con los mismos nombres) Para ajustar el modelo propuesto con la 1a fórmula propuesta tenemos: m1 &lt;- ppm(Q = puntos.2.ppp, trend = ~ Var.1 + Var.3 + I(Var.1^2) + I(Var.3^2), covariates = s.im) Para ver un resumen detallado del modelo ajustado, podemos utilizar la función summary del objeto creado m1, lo que imprimirá una tabla que muestra los coeficientes estimados, error para cada coeficiente, su significancia y otra información sobre el llamado a la función ppm y estadísticas de convergencia: ## Point process model ## Fitting method: maximum likelihood (Berman-Turner approximation) ## Model was fitted using glm() ## Algorithm converged ## Call: ## ppm.ppp(Q = puntos.2.ppp, trend = ~Var.1 + Var.3 + I(Var.1^2) + ## I(Var.3^2), covariates = s.im) ## Edge correction: &quot;border&quot; ## [border correction distance r = 0 ] ## -------------------------------------------------------------------------------- ## Quadrature scheme (Berman-Turner) = data + dummy + weights ## ## Data pattern: ## Planar point pattern: 200 points ## Average intensity 8.57 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## ## Dummy quadrature points: ## 32 x 32 grid of dummy points, plus 4 corner points ## dummy spacing: 0.1562500 x 0.1458333 units ## ## Original dummy parameters: = ## Planar point pattern: 1028 points ## Average intensity 44.1 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## Quadrature weights: ## (counting weights based on 32 x 32 array of rectangular tiles) ## All weights: ## range: [0.0076, 0.0228] total: 23.3 ## Weights on data points: ## range: [0.0076, 0.0114] total: 2.21 ## Weights on dummy points: ## range: [0.0076, 0.0228] total: 21.1 ## -------------------------------------------------------------------------------- ## FITTED MODEL: ## ## Nonstationary Poisson process ## ## ---- Intensity: ---- ## ## Log intensity: ~Var.1 + Var.3 + I(Var.1^2) + I(Var.3^2) ## Model depends on external covariates &#39;Var.1&#39; and &#39;Var.3&#39; ## Covariates provided: ## Var.1: im ## Var.2: im ## Var.3: im ## ## Fitted trend coefficients: ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.3^2) ## -5.708681e+01 4.649718e-01 1.973027e-01 -1.195420e-03 -6.753759e-04 ## ## Estimate S.E. CI95.lo CI95.hi Ztest ## (Intercept) -5.708681e+01 1.618153e+01 -88.802039552 -2.537159e+01 *** ## Var.1 4.649718e-01 1.498312e-01 0.171308062 7.586355e-01 ** ## Var.3 1.973027e-01 9.701636e-02 0.007154105 3.874513e-01 * ## I(Var.1^2) -1.195420e-03 3.794566e-04 -0.001939142 -4.516991e-04 ** ## I(Var.3^2) -6.753759e-04 3.126318e-04 -0.001288123 -6.262874e-05 * ## Zval ## (Intercept) -3.527899 ## Var.1 3.103304 ## Var.3 2.033705 ## I(Var.1^2) -3.150348 ## I(Var.3^2) -2.160291 ## ## ----------- gory details ----- ## ## Fitted regular parameters (theta): ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.3^2) ## -5.708681e+01 4.649718e-01 1.973027e-01 -1.195420e-03 -6.753759e-04 ## ## Fitted exp(theta): ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.3^2) ## 1.612543e-25 1.591969e+00 1.218113e+00 9.988053e-01 9.993249e-01 La parte del resumen del modelo ajustado que contiene los detalles de los coeficientes estimados es la que dice Fitted trend coefficients. La primera columna de esta tabla contiene los nombres de las variables, la segunda columna (Estimates) contiene el valor medio estimado de cada coeficiente. Las columnas 3-5 contienen el error estándar (S.E.), intervalo de confianza inferior y superior. La última columna (Ztest) contiene el valor de la probabilidad de que el intervalo a 95% contenga el valor de cero (0). Cuanto menos probable sea que contenga cero mejor. Las predicciones del modelo podemos verlas con la función plot: par(mfrow = c(1, 2)) plot(m1) Figura 5.10: Mapa de las predicciones del modelo. El panel izquierdo muestra la tendencia espacial y el derecho el error estándar de la tendencia estimada. 5.2.1 Selección del modelo Ahora que ya sabemos ajustar un modelo, podemos proceder a ajustar los modelos de las fórmulas alternativas: m2 &lt;- ppm(Q = puntos.2.ppp, trend = ~ Var.1 + Var.3 + I(Var.1^2) + I(Var.1^3) + I(Var.3^2), covariates = s.im) m3 &lt;- ppm(Q = puntos.2.ppp, trend = ~ Var.2 + Var.3 + I(Var.2^2) + I(Var.3^2), covariates = s.im) El dilema con el que nos enfrentamos ahora es decidir con cuál modelo nos quedaremos. Hay una serie de criterios para tomar esta decisión que tienen que ver principalmente con: El cumplimiento de los supuestos estadísticos (independencia de puntos y análisis de residuales) El balance entre complejidad (cantidad de variables) y varianza explicada Estimación correcta de los efectos (coeficientes) y su significancia Como vimos anteriormente, el primer supuesto es que los puntos deben ser independientes, por lo que podemos simular envolturas de Ripley para los modelos ajustados, y los residuales podemos analizarlos visualmente. El balance entre la complejidad y la varianza explicada podemos calcularlo con el criterio de información de Akaike. 5.2.1.1 Criterio de información de Akaike Calcular el AIC (por sus siglas en inglés), es muy fácil en R. Solamente necesitamos la función AIC, y proporcionarle los modelos cuyos criterios querramos conocer: AIC(m1) ## [1] -473.2666 AIC(m2) ## [1] -473.0841 AIC(m3) ## [1] -468.7745 La regla general es que cuanto más bajo sea el AIC, mejor, por lo que el modelo 3 (m3), parece tener la ventaja sobre el 1 y 2. 5.2.1.2 Estimación correcta de efectos Dado que los MPPs son complejos es frecuente encontrarse con modelos que no pudieron ser ajustados correctamente, o sea que la rutina de optimización pudo encontrar los valores de los parámetros y calcular su significancia estadística. Por otra parte, cuando los efectos estadísticos pudieron ser calculados, nos interesa que la mayoría de estos sean significativamente diferentes de cero (\\(P \\leq 0.05\\)) Si revisamos el resumen del modelo m2, veremos que aparecen algunos de los errores mencionados, y que resultan en la ausencia de estimaciones de significancia estadística (columna Ztest). summary(m2) ## Error in solve.default(M) : ## sistema es computacionalmente singular: número de condición recíproco = 1.96602e-20 ## Warning: Cannot compute variance: Fisher information matrix is singular ## Error in solve.default(M) : ## sistema es computacionalmente singular: número de condición recíproco = 1.96602e-20 ## Warning: Cannot compute variance: Fisher information matrix is singular ## Point process model ## Fitting method: maximum likelihood (Berman-Turner approximation) ## Model was fitted using glm() ## Algorithm converged ## Call: ## ppm.ppp(Q = puntos.2.ppp, trend = ~Var.1 + Var.3 + I(Var.1^2) + ## I(Var.1^3) + I(Var.3^2), covariates = s.im) ## Edge correction: &quot;border&quot; ## [border correction distance r = 0 ] ## -------------------------------------------------------------------------------- ## Quadrature scheme (Berman-Turner) = data + dummy + weights ## ## Data pattern: ## Planar point pattern: 200 points ## Average intensity 8.57 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## ## Dummy quadrature points: ## 32 x 32 grid of dummy points, plus 4 corner points ## dummy spacing: 0.1562500 x 0.1458333 units ## ## Original dummy parameters: = ## Planar point pattern: 1028 points ## Average intensity 44.1 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## Quadrature weights: ## (counting weights based on 32 x 32 array of rectangular tiles) ## All weights: ## range: [0.0076, 0.0228] total: 23.3 ## Weights on data points: ## range: [0.0076, 0.0114] total: 2.21 ## Weights on dummy points: ## range: [0.0076, 0.0228] total: 21.1 ## -------------------------------------------------------------------------------- ## FITTED MODEL: ## ## Nonstationary Poisson process ## ## ---- Intensity: ---- ## ## Log intensity: ~Var.1 + Var.3 + I(Var.1^2) + I(Var.1^3) + I(Var.3^2) ## Model depends on external covariates &#39;Var.1&#39; and &#39;Var.3&#39; ## Covariates provided: ## Var.1: im ## Var.2: im ## Var.3: im ## ## Fitted trend coefficients: ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.1^3) ## 61.1040359825 -1.3794197996 0.2021579948 0.0083312712 -0.0000163403 ## I(Var.3^2) ## -0.0006905730 ## ## ----------- gory details ----- ## ## Fitted regular parameters (theta): ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.1^3) ## 61.1040359825 -1.3794197996 0.2021579948 0.0083312712 -0.0000163403 ## I(Var.3^2) ## -0.0006905730 ## ## Fitted exp(theta): ## (Intercept) Var.1 Var.3 I(Var.1^2) I(Var.1^3) I(Var.3^2) ## 3.444654e+26 2.517246e-01 1.224041e+00 1.008366e+00 9.999837e-01 9.993097e-01 En comparación, el resumen del modelo 3: summary(m3) ## Point process model ## Fitting method: maximum likelihood (Berman-Turner approximation) ## Model was fitted using glm() ## Algorithm converged ## Call: ## ppm.ppp(Q = puntos.2.ppp, trend = ~Var.2 + Var.3 + I(Var.2^2) + ## I(Var.3^2), covariates = s.im) ## Edge correction: &quot;border&quot; ## [border correction distance r = 0 ] ## -------------------------------------------------------------------------------- ## Quadrature scheme (Berman-Turner) = data + dummy + weights ## ## Data pattern: ## Planar point pattern: 200 points ## Average intensity 8.57 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## ## Dummy quadrature points: ## 32 x 32 grid of dummy points, plus 4 corner points ## dummy spacing: 0.1562500 x 0.1458333 units ## ## Original dummy parameters: = ## Planar point pattern: 1028 points ## Average intensity 44.1 points per square unit ## binary image mask ## 28 x 30 pixel array (ny, nx) ## pixel size: 0.167 by 0.167 units ## enclosing rectangle: [-104.92138, -99.92138] x [25.355693, 30.02236] units ## (5 x 4.667 units) ## Window area = 23.3333 square units ## Fraction of frame area: 1 ## Quadrature weights: ## (counting weights based on 32 x 32 array of rectangular tiles) ## All weights: ## range: [0.0076, 0.0228] total: 23.3 ## Weights on data points: ## range: [0.0076, 0.0114] total: 2.21 ## Weights on dummy points: ## range: [0.0076, 0.0228] total: 21.1 ## -------------------------------------------------------------------------------- ## FITTED MODEL: ## ## Nonstationary Poisson process ## ## ---- Intensity: ---- ## ## Log intensity: ~Var.2 + Var.3 + I(Var.2^2) + I(Var.3^2) ## Model depends on external covariates &#39;Var.2&#39; and &#39;Var.3&#39; ## Covariates provided: ## Var.1: im ## Var.2: im ## Var.3: im ## ## Fitted trend coefficients: ## (Intercept) Var.2 Var.3 I(Var.2^2) I(Var.3^2) ## -3.268417e+01 2.686269e-01 2.552085e-01 -1.146632e-03 -8.348829e-04 ## ## Estimate S.E. CI95.lo CI95.hi Ztest ## (Intercept) -3.268417e+01 1.004036e+01 -52.362919381 -1.300542e+01 ** ## Var.2 2.686269e-01 9.339775e-02 0.085570710 4.516832e-01 ** ## Var.3 2.552085e-01 9.942583e-02 0.060337421 4.500795e-01 * ## I(Var.2^2) -1.146632e-03 3.973600e-04 -0.001925444 -3.678210e-04 ** ## I(Var.3^2) -8.348829e-04 3.187065e-04 -0.001459536 -2.102297e-04 ** ## Zval ## (Intercept) -3.255278 ## Var.2 2.876161 ## Var.3 2.566823 ## I(Var.2^2) -2.885626 ## I(Var.3^2) -2.619598 ## ## ----------- gory details ----- ## ## Fitted regular parameters (theta): ## (Intercept) Var.2 Var.3 I(Var.2^2) I(Var.3^2) ## -3.268417e+01 2.686269e-01 2.552085e-01 -1.146632e-03 -8.348829e-04 ## ## Fitted exp(theta): ## (Intercept) Var.2 Var.3 I(Var.2^2) I(Var.3^2) ## 6.389180e-15 1.308167e+00 1.290731e+00 9.988540e-01 9.991655e-01 No tiene alertas de errores y sí imprime la columna de significancia estadística. Con esta simple verificación concluimos que m3 es más adecuado que m1 y m2, con base en los criterios: Minimización de AIC Estimación de efectos Estimación de significancia estadística Aún así, es posible que m1 cumpla mejor con el criterio del supuesto de independencia, que veremos a continuación. 5.2.2 Verificación de supuestos de independencia Este criterio lo podemos verificar con dos pruebas adicionales: Análisis de residuales Simulación de envolturas K 5.2.2.1 Analisis de residuales En las metodologias de regresion los efectos fijos se utilizan para explicar el comportamiento promedio de una variable aleatoria. Cuando la variable aleatoria tiene una distribucion normal y calculamos la media aritmetica: \\[\\begin{equation} \\mu_X = \\sum \\frac{x_i}{n} \\end{equation}\\] y despues restamos la media aritmetica a cada uno de los valores de \\(X\\), el resultado es la misma variable con distribucion normal pero con media de cero. Para ilustrar esto, simulemos una variable de diez valores con media de 5 y desviacion estandar de 2: x &lt;- rnorm(10, mean = 5, sd = 2); x ## [1] 7.2202528 6.1047589 4.3691186 0.9106072 4.4739299 4.6441368 7.2534409 ## [8] 3.5502655 4.4711294 9.7822805 verificamos la media: ## [1] 5.277992 Para mostrar la distribucion de la variable tambien la podemos graficar e indicar donde queda la media estimada: El efecto de restar la media a todos los valores de \\(x\\) se muestra a continuacion x.0 &lt;- x - mean(x) plot(density(x.0), col = &quot;red&quot;, main = &quot;Densidad de x.0&quot;) abline(v = mean(x.0), lty = 3, col = &quot;red&quot;) Para mostrar mas objetivamente que ambas variables tienen una distribucion normal tambien podemos hacer: shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.95411, p-value = 0.7171 shapiro.test(x.0) ## ## Shapiro-Wilk normality test ## ## data: x.0 ## W = 0.95411, p-value = 0.7171 De igual manera que como acabamos de hacer al restar la media de toda la variable \\(x\\) los residuals se obtienen restando las predicciones de un modelo (lineal o de puntos, p. ej.) se restan a todos los valores de la variable dependiente. Por ejemplo, vamos a simular otras dos variables \\(x\\) y \\(y\\), de modo que: \\[ y(x) = \\alpha + \\beta x\\] \\(y\\) sea una funcion de \\(x\\). x &lt;- rnorm(100) y &lt;- rnorm(100, 10, 1) + runif(1, 2, 3) * x plot(x, y, main = &quot;&quot;, col = &quot;red&quot;) Ajustaremos el modelo lineal para estimar a \\(\\alpha\\) y \\(\\beta\\), y extraer los residuales: mod.lin &lt;- lm(y~x) resids &lt;- residuals(mod.lin) predics &lt;- predict(mod.lin) y veremos como \\(\\varepsilon = y - y(x)\\) df &lt;- data.frame(y = y, predicciones = predics, residuales = resids) knitr::kable(head(df)) y predicciones residuales 9.547765 9.873913 -0.3261483 10.919382 11.475169 -0.5557877 10.862230 10.278643 0.5835869 10.403145 11.145252 -0.7421070 6.016042 6.419692 -0.4036495 10.533279 10.254839 0.2784400 puesto que \\(y(x_i)\\) es la media de \\(y_i\\) para \\(x_i\\). De modo que del mismo modo que con la figura ??, los residuales \\(\\varepsilon\\) tienen una distribucion normal con media de cero (0): En el caso de los procesos de puntos los residuales tambien deben tener una media de cero, y ser aproximadamente normales (con varianza homogenea). A diferencia de los procedimientos de regresión lineal, los procesos de puntos no se limitan al análisis de las coordenadas del fenómeno que estamos estudiando, si no a todas las unidades espaciales donde podría estar definido el proceso de puntos. Esto implica que tenemos que evaluar los residuales en el espacio. aquí surge una pregunta muy natural, ¿cómo se evalúan los residuales de una serie de puntos discretos en una rejilla de unidades espaciales? El paquete spatstat emplea un método muy ingenioso que consiste en: Generación de un mapa de densidad del procesos de puntos analizado con una estimación de kernel Calcular la diferencia entre la densidad estimada con el modelo y la de kernel Suavizar la diferencia, promediando unidades espaciales adyacentes con una distancia similar a la utilizada en la estimación de kernel Sumar los residuales en las dimensiones \\(x\\) y \\(y\\) El resultado de este procedimiento se llama lurking plot. La función para hacer este análisis es diagnose.ppm: par(mar = c(2,2,2,2)) diagnose.ppm(m3) Figura 5.11: Gráfico de residuales suavizados del modelo 3. ## Model diagnostics (raw residuals) ## Diagnostics available: ## four-panel plot ## mark plot ## smoothed residual field ## x cumulative residuals ## y cumulative residuals ## sum of all residuals ## sum of raw residuals in entire window = -8.688e-11 ## area of entire window = 23.33 ## quadrature area = 23.33 ## range of smoothed field = [-4.133, 2.697] El primer panel (arriba, izquierdo) de este gráfico muestra el proceso de puntos analizado, seguido a la derecha, de los residuales acumulados en el eje de las \\(y\\). En el panel de abajo a la izquierda se muestran los residuales acumulados en el eje de las \\(x\\) y en el panel de abajo a la derecha, se muestran los residuales. En un escenario ideal, este último panel debe mostrar valores muy cercanos a cero. Las líneas punteadas que rodean a la suma de residuales son los límites de tolerancia que no deben ser excedidos por los residuales para cumplirse el supuesto de aleatoriedad de los residuales. Como resulta evidente, los residuales en \\(y\\) exceden el límite de tolerancia en una región. 5.2.2.2 Simulación de envolturas de Ripley Como vimos en el análisis exloratorio, podemos medir la autocorrelación para ver si de antemano necesitamos tomarla en cuenta para el análisis. La prueba que hicimos fue la de Ripley, comparando el número promedio de vecinos en función de un radio al rededor de cada punto. Aún cuando un proceso de puntos Poisson asume que los puntos son independientes entre sí, es posible que cuando detectamos autocorrelación con la prueba de Ripley, podamos generar un modelo Poisson que explique el patrón de puntos por medio de covariables. Para ver si el modelo que formulamos en efecto explica la autocorrelación, podemos usarlo para simular patrones de puntos y comparalos con la expectativa teórica. Para hacer esta comparación utilizamos la misma función que antes, pero el primer argumento es el objeto que contiene el modelo que queremos analizar. Comparemos en esta ocasión los tres modelos: K1 &lt;- envelope(m1, Kest, nsim = 39) ## Generating 39 simulated realisations of fitted Poisson model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39. ## ## Done. K2 &lt;- envelope(m2, Kest, nsim = 39) ## Generating 39 simulated realisations of fitted Poisson model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39. ## ## Done. K3 &lt;- envelope(m3, Kest, nsim = 39) ## Generating 39 simulated realisations of fitted Poisson model ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39. ## ## Done. par(mfrow = c(1, 3)) plot(K1); plot(K2); plot(K3) Figura 5.12: Gráficas de las envolturas de Ripley. Todos los modelos muestran un comportamiento adecuado, aunque m3 parece estar marginalmente más cerca de la expectativa teórica (en rojo), sin ser significativo. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

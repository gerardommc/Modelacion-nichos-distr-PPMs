La validación es un paso esencial en el proceso del desarrollo de un modelo, pues informa al usuario de la capacidad predictiva y la magnitud de los errores esperados. En modelación correlativa de nichos ecológicos hay prácticas estándar que consisten de:

1. Proponer modelo
2. Dividir presencias es datos de ajuste y de validación
3. Medir capacidad predictiva de un modelo utilizando los datos de validación
4. Seleccionar modelo que mejor prediga los datos de validación

La metodología general de modelación de nichos fue desarrollada así porque las herramientas espacialmente explícitas y estadísticamente robustas como los procesos de puntos no se habían adoptado en ecología hasta hace unos 10 años que se demostró que son equivalentes a metodologías de aprendizaje de máquinas desarrolladas específicamente para modelar distribuciones de organismos como MaxEnt [@Renner2013]. Con procesos de puntos, es posible proponer y seleccionar modelos con base en la larga tradición de modelación estadística frecuentista que ya es muy bien entendida y para la cual se han desarrollado criterios claros para medir qué tan bien un modelo representa los datos (como el de información de Akaike). Sin embargo, también se ha demostrado, que estos criterios pueden fallar cuando los datos analizados involucran correlación espacial [@Velasco2019akaike]. Por lo tanto, es recomendable continuar aplicando algunos de los métodos de validación que se comenzaron a adaptar a la modelación de nichos ecológicos desde sus etapas tempranas.

### Pruebas de validación estadística

#### Área bajo la curva

Estas tienen como objetivo dotar al usuario de los modelos de la información necesaria para establecer un balance entre distintos errores de predicción. Hasta hace unos años, la métrica de elección era el área bajo la curva del operador característico (AUC-ROC por sus siglas en inglés). Este método pretende evaluar la capacidad de un modelo de distribución de distinguir entre presencias de ausencias. Un modelo perfecto es aquel que clasifica correctamente todas las presencias como presencias y las ausencias como ausencias, lo que resulta en un AUC = 1. Cuando el modelo no distingue presencias de ausencias mejor que un predictor aleatorio AUC = 0.5. En un escenario ideal, lo datos de presencia y ausencia que se utilizan para calcular el AUC son datos que no te utilizaron para ajustar el modelo, por lo que la partición de la base de datos es necesaria. Esta prueba de desempeño predictivo, naturalmente sólo es útil cuando se cuenta con datos de ausencia verdadera y el método de modelación es adecuado para ese tipo de datos, y por lo tanto no es adecuada para medis el desemeño de modelos bsados en procesos de puntos.

Además de ser una prueba inadecuada para datos de sólo presencia, el AUC, tiene graves deficiencias para modelación de áreas de distribución. La mayor de ellas tiene que ver con el criterio espacial para asignar los puntos de ausencia, si éstos se encuentran en regiones muy lejanas a los puntos de presencia, el modelo podrá discriminarlas muy fácilmente, generando la falsa sensación de que es muy bueno para predecir [@Barve2011] (figura \@ref(Barve-area)). En este respecto, se recomienda entonces que las ausencias utilizadas provengan de regiones que sean accesibles para la especie pero no esté presente por las condiciones ambientales que presenta. Además se han propuesto una serie de adecuaciones de simulación para estimar valores de significancia estadística [@Raes2007]

```{r Barve-area, echo = F, fig.cap="Efecto el área de calibración sobre el desempeño predictivo medido con el área bajo la curva.", fig.align='center'}
knitr::include_graphics("Unidad-V/Barve-area.png")
```

#### Cociente de áreas parciales

Peterson [@Peterson2008] propuso una modificación a la AUC llamada ROC parcial, para evaluar predictores espaciales, que están basados en el porcentaje de predicción en relación al tamaño de las áreas predichas al aumentar los umbrales de binarización de un modelo. La diferencia principal entre el cociente de áreas parciales propuesto por Peterson et al. (2008), está en los ejes utilizados para estimarla. En las AUC tradicionales, se utiliza en el eje $x$ el porcentaje de presencias predichas y en el $y$ el pocentaje de ausencias clasificadas como presencias. En las ROC parciales, el eje $x$ es el porcentaje de área predicha y en el $y$ el porcentaje de presencias predichas (Figura \@ref(Peterson-parcial)).

```{r Peterson-parcial, echo = F, fig.align='center', fig.cap="Los ejes que utilizados en la estimación de las ROC parciales."}
knitr::include_graphics("Unidad-V/Peterson-parcial.png")
```

El nombre *ROC parcial* proviene del hecho de que el valor final obtenido es el cociente del área total bajo la curva entre el área que indica el ubral de predicción aleatoria, que es equivalente a la proporción de área predicha, eliminando posibles sesgos de estimación producto del posible valor que indica predicción aleatoria. Por ejemplo, si un modelo sólo predice 20% del área como favorable para la especie, y en dicha área sólo se encuentra el 20% de las presencias, 20%/20% = 1, lo que indica que el modelo es un predictor netamente aleatorio. Por el contrario, si con un 20% de área predicha, el modelo predice 40% de las presencias 40/20 = 2, indicando que el modelo es significativamente mejor que aleatorio. 

El resultado es una métrica de evaluación menos sensible al área de calibración que las AUC tradicionales y que es también más adecuada para evaluar la capacidad predictiva de modelos construidos sólo con datos de presencia [@Barve2011].

#### Tutorial de validación

Vamos a analizar primero dos estrategias de partición de datos, una aleatoria y otra espacialmente estructurada. En la partición aleatoria, los puntos de ajuste y validación se seleccionan aleatoriamente, en la espacialmente estructurada se utilizan todos los puntos de una región para ajustar un modelo y los puntos de otras regiones para validarlo. 

##### Partición aleatoria

Comenzaremos seleccionanado aleatoriamente el 70% de los puntos para ajuste y 30% para validación, una vez que tengamos el objeto de puntos crudo, sin formatear para spatstat:

```{r echo = F, message = F, warning = F}
library(spatstat) # Spatial Statistics
library(raster)

arch <- list.files("../Datos-ejemplos", ".tif", full.names = T)
s <- stack(arch)

source("../Funciones-spatstat/imFromStack.R")

s.im <- imFromStack(s)

source("../Funciones-spatstat/winFromRaster.R")

w <- winFromRaster(s)

# Importar puntos

puntos <- read.csv("../Datos-ejemplos/Puntos-tutorial-2.csv")
```

```{r echo = T}
datos.aj <- sort(sample(1:nrow(puntos), 0.7 * nrow(puntos)))
datos.val <- c(1:nrow(puntos))[ ! c(1:nrow(puntos))%in% datos.aj ]
write.csv(puntos[datos.val, ], "../Datos-ejemplos/Puntos-validacion.csv", row.names = F)

ppp.aj <- ppp(x = puntos$x[datos.aj], y = puntos$y[datos.aj],
                  window = w)
ppp.val <- ppp(x = puntos$x[datos.val], y = puntos$y[datos.val],
                  window = w)
```

En `datos.aj` estamos seleccionando al azar las filas para ajustar el modelo, mientras que en `datos.val`, estamos extrayendo todas las filas que no se seleccionaron para ajustar. Una vez identificados los puntos que se utilizarán para ajustar y validar, se utilizan los índices para crear los dos objetos que se utilizarán para ajustar y validar respectivamente. El procedimiento para ajustar un modelo con la base de datos de ajuste es el mismo que anteriormente. Nos saltamos la parte del diagnóstico pues el interés ahora se centra en medir la capacidad predictiva. Una vez ajustado el modelo necesitamos guardar las predicciones en formato raster

```{r echo = T, warning=FALSE}
m.val <- ppm(Q = ppp.aj, # Proceso de puntos en formato ppp
             trend = ~ Var.2 + Var.3 + I(Var.2^2) + I(Var.3^2), #  Formula del modelo seleccionado     
             covariates = s.im)
pred.val <- predict(m.val, dimyx = c(28, 30), type = "trend")
pred.val.r <- raster(pred.val)
writeRaster(pred.val.r, "../Datos-ejemplos/Res/Predicciones-validacion-aleat", "GTiff", overwrite = T)
```

Una vez guardadas las predicciones del modelo, necesitamos instalar y cargar el paquete `kuenm` que contiene la función para calcular las ROC parciales. La instalación es un poco diferente pues requiere instalar el paquete `devtools` porque `kuenm` no está almacenado en los repositorios de **R**.

```{r eval=F, echo=T}
devtools::install_github("marlonecobos/kuenm")
```

Una vez, instalado, continuamos evaluando el modelo con el cociente de áreas del ROC:

```{r echo = T}
puntos.val <- read.csv("../Datos-ejemplos/Puntos-validacion.csv")
model <- raster("../Datos-ejemplos/Res/Predicciones-validacion.tif")
proc <- kuenm::kuenm_proc(occ.test = puntos.val,
                  model = model,
                  threshold = 5,
                  rand.percent = 50,
                  iterations = 39, parallel = F)
```

para ver los resultados:

```{r echo = T}
proc$pROC_summary
```

#### Datos de validación estructurados espacialmente

Para muestrear los puntos con una estructura espacial, necesitamos diseñar un criterio. Una de las alternativas más populares es la creación de una cuadrícula de ajedrez, y utilizar los puntos en cuadros blancos para ajustar y los puntos en cuadros negros para validar. La generación de los cuadros está implementada en el paquete `dismo`, en la función `gridSample`:

```{r echo=T}
p.bl <- dismo::gridSample(puntos, r = s[[1]], n = 50, chess = "white")
p.ne <- dismo::gridSample(puntos, r = s[[1]], n = 50, chess = "black")
```

Como muestra del resultado:

```{r echo = T, fig.align='center', fig.height=4.5, fig.width=4.5}
plot(s[[1]]); points(p.bl)
plot(s[[1]]); points(p.ne)
```

A partir de este punto el proceso de validación es el mismo que con las muestras aleatorias, aunque es preferible seguir el siguiente proceso:

1. Ajustar modelo con puntos en cuadrantes blancos y validar con puntos en cuadrantes negros
2. Ajustar modelo con puntos en cuadrantes negros y validar con puntos en cuadrantes blancos

Antes de proponer un modelo para los datos de ocurrencia, es buena práctica realizar un análisis exploratorio. En realidad, el modelo que propongamos dependerá en buena medida de este análisis exploratorio. Los análisis exploratorios que yo hago comprenden una serie de pasos:

1. Probar si los datos de ocurrencia cumplen son independientes unos de otros o si están autocorrelacionados
2. Ver la respuesta de la intensidad de puntos ante las diferentes variables ambientales que queremos incorporar en el modelo
3. Medir la correlación entre las diferentes variables
4. Con base en los puntos 2 y 3 proponer un conjunto de modelos alternativos (más sobre este punto abajo)

### Independencia y autocorrelación

Se dice que los diferentes puntos son independientes entre sí, si el número de vecinos promedio de cada punto como función de la distancia, sigue una distribución Poisson (figura \@ref(fig:Cuenta-vecinos)). Con esto en mente hay varios escenarios posibles, que en promedio cada punto tenga más vecinos de lo esperado o que tenga menos. En el primer caso, se dice que los puntos están agregados, pues un punto tiende a atraer a otros. En el segundo, los puntos están segregados, o sea que un punto tiende a mantener a otros puntos lejos de sí. El primer caso, de independencia, suele ocurrir cuando los puntos están distribuidos aleatoriamente (figura \@ref(fig:Segreg-agreg)).

```{r Cuenta-vecinos, echo = F, fig.align='center', fig.cap="Número de vecinos como función de la distancia en un proceso de puntos."}
knitr::include_graphics("Unidad-II/Cuenta-vecinos.png")
```


```{r Segreg-agreg, echo = F, fig.align='center', fig.cap="Ejemplo de procesos de puntos de izquierda a derecha: segregado, aleatorio y agregado (reproducido de Baddeley y Rubak 2016)"}
knitr::include_graphics("Unidad-II/Ejemplo-procesos.png")
```

Existe una serie de pruebas gráficas y estadísticas para medir autocorrelación. Aquí nos enfocaremos en el uso de la prueba de envolturas *K* de Ripley. La implementación de esta prueba en `spatstat` genera unos intervalos de confianza alrededor de la expectativa del número de vecinos por medio de simulación. La figura \@ref(fig:K-env) muestra los tres escenarios de segregación, aleatorio y agrecación.

```{r K-env, echo = F, fig.align='center', fig.cap="Gráfica de la prueba *K* de Ripley implementada en `spatstat`. De izquierda a derecha: funciones de Ripley para puntos segregados, aleatorios y agregados."}
knitr::include_graphics("Unidad-II/K-Ripley.png")
```

#### Análisis de autocorrelación en **R** con `spatstat`

Haremos este análisis de autocorrelación con el mismo proceso de puntos que formateamos anteriormente. La función de `spatstat` para la prueba de Ripley es `envelope`, y los argumentos que requiere son 1) el proceso de puntos a analizar, 2) la función con que se medirá autocorrelación (`Kest` para $K$ de Ripley) y 3) el número de simulaciones. Normalmente, para un nivel de significancia $P=0.05$, se utilizan 39 simulaciones, el cual deberá aumentar si el umbral de significancia buscado es más estricto ($P = 0.01$, p. ej.).

```{r echo = T}
K <- envelope(puntos.ppp, fun = Kest, nsim = 39)
```

Mientras la función corre, **R** imprime la última simulación completada, y el objeto que se obtiene puede graficarse con el método por defecto `plot`:

```{r echo = T, fig.align='center', fig.cap="Gráfica de la función *K* de Ripley para el proceso de puntos analizado. Las sombras en gris muestran los intervalos de confianza al 95%. La línea roja a guiones representa la expectativa teórica (*K* teórica) en caso de que el proceso de puntos sea aleatorio, y la línea negra sólida es la función de Ripley para el proceso de puntos (la *K* observada). El eje de las *x* representa distancia (en grados) y las *y* el número promedio de vecinos de cada punto.", fig.width=6, fig.height=6}
plot(K)
```

### Análisis gráfico de las respuestas al medio ambiente.

Para este análisis simularé una base de datos de ocurrencia donde la probabilidad de observarlos sea inversamente proporcional a la distancia de una centroide pre definido. Con base en ello, podremos ver cómo cambia la intensidad de puntos en realción a los diferentes valores de cada variable.

#### Simulación de datos de presencia

Utilizaré las mismas variables que para el ejercicio anterior de formateo, y el centroide estará localizado en la media aritmética de cada capa:

```{r echo = T}
centroide <- cellStats(s, mean)
```

Para calcular la distancia al centroide, necesitamos la covarianza entre las diferentes capas, de modo que la calculamos con la función `cov`. Hay implementaciones más robustas en el paquete `MASS`, para nuestros propósitos pedagógicos `cov` es suficiente. Comenzamos entonces, transformando el stack en una tabla:

```{r echo = T}
s.df <- data.frame(rasterToPoints(s))
covar <- cov(s.df[, 3:5])
```

Posteriormente, utilizando el centroide y la matriz de covarianza, generamos las distancias utilizando las tres columnas del objeto `s.df` que contienen los valores de las variables ambientales:

```{r echo = T}
md <- mahalanobis(s.df[, 3:5], center = centroide, cov = covar)
```

Y transformamos las distancias al centroide en una capa raster:

```{r Distancia, echo = T, fig.height=5, fig.width=5, fig.align='center', fig.cap="Distancia Mahalanobis al centroide de las capas."}
md.r <- rasterFromXYZ(data.frame(s.df[, 1:2], md))
plot(md.r)
```

Para simular las ocurrencias, transformaré la capa de distancias exponencialmente, para obtener una superficie probabilística:

```{r Favorabilidad, echo = T, fig.height=5, fig.width=5, fig.align='center', fig.cap="Distancia Mahalanobis transformada exponencialmente para simular presencias. Verde indica mayor probabilidad de ocurrencia."}
md.exp <- exp(-0.5*md.r)
plot(md.exp)
```

Para simular las presencias usaré el mismo método que anteriormente, pero en esta ocasión la probabilidad determinará las celdas en que habrá puntos:

```{r puntos, echo = T}
set.seed(182)
puntos.2 <- dismo::randomPoints(mask = md.exp,
                                n = 200,
                                prob = T)
puntos.2 <- data.frame(puntos.2)
puntos.2$x <- puntos.2$x + rnorm(200, 0, 0.05)
puntos.2$y <- puntos.2$y + rnorm(200, 0, 0.05)

plot(md.exp); points(puntos.2)
```
#### Graficación y análisis de las respuestas

Para continuar con el análisis, necesitamos formatear el objeto `puntos.2` como `ppp`:

```{r echo = T}
puntos.2.ppp <- ppp(x = puntos.2$x,
                  y = puntos.2$y,
                  window = win,
                  check = F)
```

Recordemos que el objeto `win` lo generamos en la sección de formateo de este tutorial. Para el análisis de las respuestas necesitamos crear otro objeto, que contiene los conteos de cuadratura, es decir, cuántas presencias por unidad espacial, con la función `pixelquad` que requiere de dos argumentos, el proceso planar de puntos y la ventana de trabajo en formato `owin`:

```{r echo = T}
Q <- pixelquad(X = puntos.2.ppp, W = as.owin(win))
```

Dado que las capas ya están también formateadas como `im`, podemos ahora sí continuar con el análisis de las respuestas con la función `plotQuantIntens`. Esta función generará unos gráficos en pdf que deberemos revisar después de correrla. Para cargar la función, haremos igual que con las anteriores. Puedes descargar la función [aquí](Funciones-spatstat/plotQuantIntens.R). Esta función requiere de varios argumentos:

1. La lista de imágenes que se usarán para ver cómo cambia la intensidad de puntos en relación a cada variable
2. El número de cuantiles en que se cortará cada variable para representar la intensidad en el espacio
3. El objeto de cuadratura (`Q`)
4. El objeto con los puntos en formato `ppp`
5. El nombre del directorio donde se guardará el archivo pdf
6. El nombre del archivo

```{r echo = T, message = F, warning = F}
source("../Funciones-spatstat/plotQuantIntens.R")

plotQuantIntens(imList = s.im,
                noCuts = 5,
                Quad = Q,
                p.pp = puntos.2.ppp,
                dir = "",
                name = "Responses-centroid")
```

El [archivo de gráficas](Unidad-II/Responses-centroid.pdf) que produce `plotQuantIntens` muestra en cada panel:

1. La variable analizada con el número de puntos en cada región de valores especificada por el argumento `noCuts` o número de cortes

2. La variable analizada con el proceso de puntos sobrepuesto

3. La respuesta de la intensidad de puntos a la variable analizada. En el eje de las $x$ (horizontal), el valor de la variable, y en el eje de las $y$ (vertical) la intensidad (número) de puntos.

La idea de este análisis es que podamos identificar *a priori* qué variables podemos incluir en el modelo y con qué tipo de relación. Por ejemplo, las variables 2 y 3 tienen respuestas claramente con forma de parábola invertida o de campana con uno que otro "tope", por lo que podemos utilizar una fórmula polinomial de $2^o$ grado (figura \@ref(fig:Cuadratica)). Para la variable 1, sin embargo parece haber una región hacia el extremo derecho del eje $x$ en la cual la intensidad vuelve a incrementar. Dado la intensidad de puntos tiende a aumentar nuevamente podríamos utilizar un término cúbico, pues una ecuación cúbica puede adquirir esta forma (figura \@ref(fig:Cubica)).

```{r Cuadratica, echo=T,  fig.height=5, fig.width=5, fig.align='center', fig.cap="Ecuación polinomial de 2o grado exponenciada."}
curve(exp(1 + x - x^2), from = -3, 3)
```
Por otro lado, una ecuación polinomial de $3^{er}$ grado:

```{r Cubica, echo = T, fig.align='center', fig.cap="Ecuación polinomial de 3er grado exponenciada.", fig.width=5, fig.height=5}
curve(exp(1+ x - 2*x^2 + x^3), from = -1.5, to = 1.4 )
```

### Midiendo la correlación entre variables independientes

Cuando hacemos un análisis de regresión, en deseable que todas las variables independientes que incluyamos en un modelo sean ortogonales, es decir que no estén correlacionadas, que no sean predictoras una de otra. Al incluir variables independientes correlacionadas creamos un problema en el que no es posible medir la varianzade la variable dependiente que explican. Un ejemplo análogo en ANOVA, sería un experimento de dos factores con dos niveles cada uno, y por lo tanto para poder entender el efecto de cada factor con sus niveles necesitaríamos cuatro unidades experimentales como mínimo, 2 niveles del factor I $\times$ 2 niveles del factor II. Si los factores fueran colineales, implicaría que sólo tendríamos por ejemplo, nivel $A$ del factor I con nivel $a$ del factor II, y nivel $b$ del factor I con nivel $b$ del factor II, siendo que requerimos de las combinaciones:$Aa, Ab, Ba$ y $Bb$. 

Para medir la correlación entre pares de variables raster estimamos el coeficiente de correlación de Pearson, con la función `pairs` del paquete `raster`. Esta función compara todas las posibles combinaciones de pares de variables en el stack y produce un gráfico de dispersión para cada combinación:

```{r correl, echo = T, fig.align='center', fig.cap="Prueba de correlación para todos los pares de variables.", fig.width = 7, fig.height = 7}
pairs(s)
```

Como podemos ver, de las variables propuestas sólo podemos utilizar dos de ellas en el mismo modelo, puesto que `Var.1`y `Var.2` están correlacionadas. La correlación entre ellas es lineal, como puede verse en el gráfico de dispersión (1a columna 2a fila).

No hay una regla que indique qué coeficiente de correlación es aceptable para incluir en un mismo modelo, sin embargo, mientras más cercano a $0$ y lejos de $1$ ó $-1$ es mejor. Ciertamente $0.76$, puede ser considerada como alta correlación. Como regla general personal, si el número de variables con respuestas claras observadas con `plotQuantIntens` alto, podemos utilizar todas aquellas con $r < 0.5$, si el número de variables es pequeño podemos permitirnos un poco más de libertad e incluir en un mismo modelo todas aquellas con $r \leq 0.7$.

### Propuesta de modelos alternativos

Anteriormente vimos que la intensidad de puntos (figura \@ref(fig:puntos)) tiene una clara res puesta de campana en relación a `Var.2` y `Var.3`, y posiblemente `Var.1`. Por ello podemos proponer una serie de funciones polinomiales de 2o grado donde sólo estén presentes `Var.1` y `Var.3`, y `Var.2` y `Var.3`. Recordemos que con base en el análisis de correlación no debemos incluir a `Var.1` y `Var.2` en el mismo modelo (figura \@ref(fig:correl)). De modo que, en sintaxis de **R**, las fórmulas propuestas del modelo (incluyendo una de 3er grado para `Var.1`):

1. `~ Var.1 + Var.3 + I(Var.1^2) + I(Var.3^2)`
2. `~ Var.1 + Var.3 + I(Var.1^2) + I(Var.1^3) + I(Var.3^2)`
3. `~ Var.2 + Var.3 + I(Var.2^2) + I(Var.3^2)`

La decisión de cuál de las fórmulas propuestas será la final con que trabajaremos, es material del próximo capítulo, por el momento veremos cómo se ajusta un `ppm`.

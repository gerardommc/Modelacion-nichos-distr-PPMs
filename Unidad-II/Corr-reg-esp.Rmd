### Correlación espacial

La correlación mide la similitud entre dos variables aleatorias, comparando sus varianzas respectivas. Es una prueba muy sencilla, pero que tiene muchas fallas, por ejemplo, no podemos saber si la correlación es causal. Veamos dos ejemplos de correlación entre dos variables ambientales, uno donde hay una alta correlación y otro donde no la hay.


```{r echo = FALSE}
knitr::include_graphics("Unidad-II/Correlated.png")
```


```{r Alta-corr, echo = F, fig.cap="Variables altamente correlacionadas. Los mapas muestran los valores de dos capas ráster, y la gráfica de dispersión contiene los valores de cada píxel, en el eje *x*, la variable de la izquieda, y el eje *y* la variable de la derecha.", fig.height=4.5, fig.width=4.5}
x1 <- rnorm(2500, 100, 20)
x1.1 <- x1 + rnorm(2500, -20, 1)
plot(x1, x1.1, xlab = "Variable 1", ylab = "Variable 2", pch = 20)
```

```{r echo=FALSE}
knitr::include_graphics("Unidad-II/Uncorrelated.png")
```


```{r Baja-corr, echo=FALSE, fig.cap="Variables pobremente correlacionadas.", fig.height=4.5, fig.width=4.5}
x2 <- rnorm(2500, 10, 20)
x2.1 <- rnorm(2500, -20, 20)
plot(x2, x2.1, xlab = "Variable 1", ylab = "Variable 2", pch = 20)
```

Claramente en el ejemplo de la figura \@ref(fig:Alta-corr), una de las variables predice a la otra, pero no sabemos cuál produce a cuál, o si ambas sor producidas por otra variable que no se ha medido. En resumidas cuentas, la correlación no se puede utilizar para analizar causalidad, lo cual es común a todos los análisis estadísticos.

#### Breve recordatorio del cálculo de la correlación

Como ya han de saber, la correlación es una prueba de estadística frecuentista paramétrica. Como tal, consiste en una serie de cáculos aritméticos para obtener un parámetro $r$:

\begin{equation}
r = \frac{\sum x_1 x_2}{\sqrt{\sum x_1^2 \sum x_2^2}}
\end{equation}

donde $x_1$ y $x_2$ son las dos variables centradas (variable menos la media aritmética), cuya correlación queremos medir. Esta prueba de correlación se llama de **Pearson**, y existen algunas modificaciones para datos de otra naturaleza como los ordinales, para lo cual se utiliza la correlación de **Spearman**. El coeficiente de correlación de esta última se denota con $\rho$.

La correlación de Spearman es menos sensible que la de Pearson a correlaciones no lineales, por lo que si la gráfica de las variables $x_1, x_2$ no forma una línea recta como en \@ref(fig:Alta-corr), se puede probar con la correlación de Spearman.

#### Correlación en R

Medir la correlación entre dos variables espaciales en **R** es muy fácil, aunque antes de proseguir, debemos aprender a manejar los datos espaciales en **R**. Como ya han de saber, cuando no podemos hacer algo en R básico, hay que echar un vistazo a las librerías. Las más potentes para manejar datos espaciales son `raster` y `rgdal`. La primera, como su nommbre indica, sirve principalmente para imágenes, la segunda, para vectores y puntos. Ambas son compatibles, es decir se pueden hacer procesos espaciales combinando los objetos de **R** que ambos paquetes utilizan.

Vamos a leer un raster con la paquetería `raster` y la función del mismo nombre:

```{r echo = T}
library(raster)
r <- raster("Capas-ejemplo/bio1.tif")
```

El nombre del achivo es `bio1.tif` y está contenido en la carpeta "Capas ejemplo". Para hacer una prueba de correlación, necesitamos leer otra capa:

```{r echo = T}
r1 <- raster("Capas-ejemplo/bio11.tif")
```

y las vamos a agrupar en un objeto para hacer la prueba de correlación con la función `pairs` del mismo paquete `raster`:

```{r echo = T, fig.height=4.5, fig.width=4.5}
st <- stack(r, r1)
pairs(st)
```

Como podemos ver, la función `pairs` hace un gráfica con los histogramas de cada variable (en verde), el gráfico de dispersión, y el coeficiente de correlación de Pearson. Vamos a ver entonces qué está haciedo la función `pairs`

Vamos a transformar sl objeto `st` en una tabla con dos columnas para las coordenadas geográficas de cada píxel y otras dos para los valores de cada variable:

```{r echo = T}
t1 <- rasterToPoints(st)
t1 <- data.frame(t1)
knitr::kable(head(t1))
```

Como sabemos el coeficiente de correlación de Pearson utiliza dos cálculos, $\sum x_1 x_2$ y $\sqrt{\sum x_1^2 \sum x_2^2}$. Vamos a hacerlos a continuación:

```{r echo = T}
x1 <- t1$bio1; x2 <- t1$bio11

x1 <- x1-mean(x1); x2 <- x2-mean(x2)

calc.1 <- sum(x1 * x2)
calc.2 <- sqrt(sum(x1^2) * sum(x2^2))

coef.pearson <- calc.1/calc.2
coef.pearson
```

### Regresión

La regresión lineal es un procedimiento relativamente similar a la correlación, aunque la principal diferencia está en que la regresión estima parámetros de una ecuación lineal que se puede usar para *predicción*, y para identificar variables que explican el comportamiento de los datos que hemos colectado o generado. En contraste la correlación sólo nos sirve para ver si dos variables aleatorias se predicen mutuamente, con qué grado de precisión (coeficiente de correlación), y el sentido (positivo o negativo).

En una regresión lineal, estimamos una serie de parámetros que corresponden con las constantes de una ecuación lineal:

$$
y(x) = \alpha + \beta x
$$
donde $\alpha$ es el intercepto o el valor de $y$ cuando $x=0$, y $\beta$ es la pendiente de $y$ con respecto de $x$, es decir cuánto cambia $y$ si $x$ aumenta en una unidad.

Otro parámetro importante que estimamos en una regresión lineal es $r^2$, y como su nombre lo indica es el cuadrado del coeficiente de correlación $r$.

Cuando hacemos modelación estadística, es importante estar al tanto de algunos supuestos. En regresión lineal, estos son algunos importantes:

1. Las observaciones de $y$ son independientes entre sí
2. La relación entre $x$ y $y$ es lineal
3. $y$ tiene una distribución unimodal y con varianza homogénea (igual por arriba y abajo de la media).